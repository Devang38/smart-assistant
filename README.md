Smart Assistant for Research Summarization

ğŸš€ Overview

This is an AI-powered smart assistant that uses OpenAI's GPT model to summarize, understand, and interact with documents (PDF or TXT). Designed for research-heavy contexts like academia, legal, or technical fields, it provides:

ğŸ“„ Automatic summarization (â‰¤ 150 words)

â“ Free-form question answering

ğŸ¯ Logic-based question generation

âœ… Answer evaluation with justifications

All responses are grounded in the actual document content using OpenAI's large language models (GPT-4 / GPT-3.5).

ğŸ”§ Setup Instructions

1. Clone the Repository

git clone https://github.com/Devang38/smart-assistant.git
cd smart-assistant

2. Create & Activate Virtual Environment

python -m venv venv
venv\Scripts\activate  # Windows
# OR
source venv/bin/activate  # Linux/Mac

3. Install Requirements

pip install -r requirements.txt

4. Set Your OpenAI API Key

Create a .env file in the root directory:

OPENAI_API_KEY=your-api-key-here

5. Run the App

uvicorn backend:app --reload

Visit http://127.0.0.1:8000 in your browser.

âš™ï¸ Application Architecture

graph TD
    A[User Uploads PDF/TXT] --> B[Text Extraction (PyMuPDF or Plain Read)]
    B --> C[Auto Summary (OpenAI GPT)]
    B --> D[QA Mode / Challenge Mode]
    D --> E[Prompt Engine with Document Context]
    E --> F[OpenAI GPT Model (GPT-4)]
    F --> G[Answer/Justify/Evaluate]
    G --> H[Frontend (Streamlit/React/Gradio)]

Components:

Frontend: Built with Streamlit or React/Gradio

Backend: FastAPI-based API

Model: OpenAI GPT (via openai SDK)

Storage: In-memory context + file handling

ğŸ¯ Functional Features

1. Document Upload

Accepts .pdf and .txt files

Extracts and preprocesses text

2. Auto Summary

Summarizes document into â‰¤ 150 words using GPT-4

3. Ask Anything

Accepts user queries

Sends document + query to OpenAI GPT

Returns response with document-based justification

4. Challenge Me

Generates 3 logical/comprehension-based questions

Accepts user answers

Evaluates correctness + provides reference justification

ğŸ’¡ Bonus Features (Planned/Optional)

âœ… Memory: Track previous interactions using session state

ğŸ“ Snippet Highlighting: Extract specific paragraph from text as justification

ğŸ§  Chain-of-Thought prompting for improved logical reasoning

ğŸ§ª Sample Prompt Format

prompt = f"""
You are a document-aware research assistant. Use the following document:
{text}

Question: {question}
Answer based on the above content. Mention the paragraph or section that supports your answer.
"""

ğŸ“¦ Tech Stack

Frontend: Streamlit 

Backend: FastAPI

AI Model: OpenAI GPT-3.5 / GPT-4

Document Parsing: PyMuPDF, Python File I/O

Language: Python 3.10+

[Frontend: Streamlit]
   |
   â”œâ”€â”€ Upload PDF/TXT
   â”œâ”€â”€ Display Summary
   â”œâ”€â”€ Ask Anything (QA input box)
   â””â”€â”€ Challenge Me (Generated Q&A with feedback)
       â†“
[Backend: FastAPI / LangChain]
   |
   â”œâ”€â”€ PDF/TXT Reader
   â”œâ”€â”€ Summarizer (LLM or Extractive)
   â”œâ”€â”€ Vector Indexing (FAISS / ChromaDB)
   â”œâ”€â”€ QA using Retrieved Chunks
   â”œâ”€â”€ Question Generator (LLM)
   â”œâ”€â”€ Answer Evaluator (Cosine Similarity + Keyword Match)
   â””â”€â”€ Justification Finder (Span/Chunk Matching)


ğŸ“ Project Structure

smart-assistant/
â”œâ”€â”€ app.py                # Main Streamlit app
â”œâ”€â”€ backend.py            # FastAPI endpoints (QA, Challenge, Summarization)
â”œâ”€â”€ document_loader.py    # PDF/TXT parsing & chunking
â”œâ”€â”€ qa_logic.py           # QA, question generation & evaluation logic
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸ“Œ License

This project is licensed under the MIT License.

ğŸ“ Future Enhancements

Add embeddings-based retrieval using langchain or llama-index

Enable persistent chat memory

Deploy to cloud via Render / Vercel / Streamlit Cloud

Made by Devang Yadav

